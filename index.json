[{"body":"","link":"https://www.professionaldev.pro/","section":"","tags":null,"title":""},{"body":"","link":"https://www.professionaldev.pro/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"In previous discussions, we established that bulk insert and multi-value batched inserts represent the fastest methods for inserting data into an MS SQL Server database. However, before concluding this series, let's explore another option.\nMS SQL Server has introduced the parameter useBulkCopyForBatchInsert, which automatically activates the bulk API for batched inserts. This functionality operates at the database driver level, enabling it to work seamlessly with both regular parameterized inserts and Hibernate inserts.\nIn this post, we will examine the performance impact of enabling this parameter on Hibernate inserts and JDBC inserts. We will compare them with regular batch inserts, manual Bulk API usage, and multi-value JDBC inserts.\nThe code utilized for these examples remains consistent with that used in previous posts and will not be reiterated.\nI conducted tests to insert 1,000,000 Person objects with various batch sizes (100, 1,000, 10,000, and 20,000). Each test was executed 10 times, and the median time was calculated. With the parameter enabled, the results are as follows:\nMethodName MedianDuration BatchSize Jdbc Template 5752 20000 Jdbc Template 5897 10000 Jdbc Template 7361.5 1000 Hibernate Persist 8019.5 10000 Hibernate Persist 8025.5 20000 Hibernate Persist 9491 1000 Jdbc Template 39273.5 100 Hibernate Persist 41904 100 The JdbcTemplate insert remains faster by approximately 2 seconds for each batch size, even with a batch size of 100. This demonstrates that Hibernate introduces overhead in processing entities, likely stemming from entity state tracking. However, it's noteworthy that this overhead remains constant regardless of the batch size used.\nNow, let's compare these results with regular batch inserts. To provide clearer insights, I'll present two separate tables comparing the performance of Hibernate inserts and JDBC inserts. Hibernate insert performance:\nBatch Size Bulk Api Median Duration (ms) Batched Median Duration (ms) 10000 8019.5 16198.5 20000 8025.5 16186 1000 9491 16825 100 41904 21830 JDBC insert performance:\nBatch Size Bulk Api Median Duration (ms) Batched Median Duration (ms) 20000 5752 14118.5 10000 5897 14320 1000 7361.5 14683.5 100 39273.5 16981.5 With the JDBC batching parameter enabled, we observe an average performance improvement of around 2 times. Its behavior resembles that of the Bulk API discussed in the previous post. For larger batch sizes and datasets, the Bulk API outperforms, but for smaller batches, it exhibits poorer performance compared to regular batch inserts. This is logical, considering that enabling the batching parameter essentially utilizes the Bulk API in the background.\nNow, how does this relate to multi-value inserts from the previous post?\nThe best-performing multi-value insert method had a batch size of 100 and 2,000 bind parameters (inserting 400 persons per insert statement). It achieved inserting 1,000,000 person objects into the database in 4.175 seconds (median duration), which is still superior to our best result with the bulk API parameter (5.752 seconds). Although the performance difference is not significant, the multi-insert method appears to be more advantageous from a performance standpoint, especially for small batches.\nFurthermore, with manually implemented Bulk API inserts taking 3.188 seconds, it's evident that manual implementation outperforms the bulk API parameter implementation. However, for this volume of processed data, the difference isn't substantial, as time increases linearly in both cases.\nConclusion We can observe that the bulk API can substantially enhance performance for bulk inserts involving large amounts of data.\nIt's important to highlight that even when enabling this parameter, the bulk API will only function if specific conditions are met. These conditions can be found on the following Microsoft page: Using bulk copy API for batch insert operation\nI'd also like to mention that I encountered an issue with inserts and the bulk API parameter when the columns of a database table are ordered differently from the entity fields. You can find further details about this problem at the following link: Bulk Copy cannot handle inserts with subset or differently ordered columns. This issue has been resolved in the latest version of the Java MS SQL Server JDBC driver.\nThis parameter has limited utility; avoid enabling it blindly without first assessing the benefits and drawbacks it may bring to your application.\nFor updates, you can follow me on Twitter or LinkedIn.\n","link":"https://www.professionaldev.pro/post/java/mssqlserver/fast_insert_part4/","section":"post","tags":["java","ms sql server"],"title":"Fastest Way To Insert the Data in MS SQL – Part 4 – Turning on the `useBulkCopyForBatchInsert` Parameter"},{"body":"","link":"https://www.professionaldev.pro/tags/java/","section":"tags","tags":null,"title":"java"},{"body":"","link":"https://www.professionaldev.pro/categories/java/","section":"categories","tags":null,"title":"java"},{"body":"","link":"https://www.professionaldev.pro/tags/ms-sql-server/","section":"tags","tags":null,"title":"ms sql server"},{"body":"","link":"https://www.professionaldev.pro/categories/sql/","section":"categories","tags":null,"title":"sql"},{"body":"","link":"https://www.professionaldev.pro/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"In a previous post, we concluded that the Bulk API insert is the fastest way to insert data into an MS SQL Server database if the batch size is reasonably large. For Jdbc Template inserts, we created a regular insert statement in the form of insert into table (c1, c2, c3) values (v1, v2, v3), where c1, c2, and c3 are column names, and v1, v2, and v3 are values.\nFor 100,000 Person objects with a batch size in the range of 1,000-10,000, we concluded that Bulk API insert is four to five times faster than JDBC Template. However, with MS SQL Server, JDBC Template has one more potential speedup that it can utilize, possibly competing with Bulk API and proving useful in certain situations.\nIf we check the MS SQL Server official syntax for the insert statement, we can see that it supports multiple values inserts in one statement. This means that if we want to insert multiple values into the same table, instead of sending multiple insert statements, we can do that in a single insert statement. The insert statement will take the following form: insert into table (c1, c2, c3) values (v1, v2, v3), (v4, v5, v6).... The maximum number of value bindings that can be included in one statement is 2,100, consistent with any other MS SQL Server statement.\nHow multiple values insert impact performance Firstly, we will compare this multiple value insert with a single value insert and check if increasing the number of value bindings per insert statement improves performance.\nThe following code is used to perform the insertion of 1,000,000 persons. It is very similar to the code from Part 2 except that it now generates multi-value insert statements.\n1public void insertPeople(TableDescriptor\u0026lt;Person\u0026gt; tableDescriptor, 2 List\u0026lt;Person\u0026gt; people, 3 int batchSize, 4 int objectsPerInsert) { 5 if (people.isEmpty()) return; 6 objectsPerInsert = Math.min(objectsPerInsert, people.size()); 7 8 List\u0026lt;List\u0026lt;Person\u0026gt;\u0026gt; partitions = Lists.partition(people, objectsPerInsert); 9 if (partitions.size() == 1 || partitions.get(partitions.size() - 1).size() == objectsPerInsert) { 10 persist(tableDescriptor, batchSize, objectsPerInsert, partitions); 11 } else { 12 List\u0026lt;List\u0026lt;Person\u0026gt;\u0026gt; equalSizePartitions = partitions.subList(0, partitions.size() - 1); 13 persist(tableDescriptor, batchSize, objectsPerInsert, equalSizePartitions); 14 List\u0026lt;List\u0026lt;Person\u0026gt;\u0026gt; lastPartition = Collections.singletonList(partitions.get(partitions.size() - 1)); 15 persist(tableDescriptor, 1, lastPartition.size(), lastPartition); 16 } 17} 18 19private void persist(TableDescriptor\u0026lt;Person\u0026gt; tableDescriptor, int batchSize, int objectsPerInsert, List\u0026lt;List\u0026lt;Person\u0026gt;\u0026gt; partitions) { 20 String insertQuery = getInsertQuery(tableDescriptor, objectsPerInsert); 21 jdbcTemplate.batchUpdate(insertQuery, 22 partitions, 23 batchSize, 24 (ps, argument) -\u0026gt; BulkPersistCustomRepositoryImpl.insertBatchData(tableDescriptor, ps, argument)); 25} 26 27public \u0026lt;T\u0026gt; void insertBulk(TableDescriptor\u0026lt;T\u0026gt; tableDescriptor, 28 List\u0026lt;? extends T\u0026gt; data, 29 int batchSize) { 30 if (data.isEmpty()) return; 31 String insertQuery = getInsertQuery(tableDescriptor, 1); 32 jdbcTemplate.batchUpdate(insertQuery, 33 data, 34 batchSize, 35 (ps, argument) -\u0026gt; { 36 List\u0026lt;ColumnDescriptor\u0026lt;T\u0026gt;\u0026gt; columns = tableDescriptor.getColumns(); 37 for (int i = 0; i \u0026lt; columns.size(); i++) { 38 ColumnDescriptor\u0026lt;T\u0026gt; columnDescriptor = columns.get(i); 39 ps.setObject(i + 1, columnDescriptor.getObjectFieldValue(argument)); 40 } 41 }); 42} I ran multiple tests with different batch sizes and the number of people per insert. Each person has five fields, so the number of binding values per insert is actually the number of people per insert times five.\nThe results:\nNumber of people per insert Batch size Median duration (ms) 400 100 4175 400 10 4275 400 1000 4348 400 10000 4697 100 1000 5335 100 100 5359 100 10 5585 100 10000 5873 10 10000 6302.5 10 1000 6331 10 100 6512 10 10 8027.5 1 10000 14792.5 1 1000 15222 1 100 17812.5 1 10 25252 As we can observe, the number of bound values per insert has a significant impact on performance. It has an even greater performance impact than the batch size, which is, for me at least, very surprising. The more parameters we bind, the better the performance we achieve. Interestingly, the best performance is with a batch size of 100.\nMulti-value insert vs Bulk API After concluding that we have much better performance with multi-value inserts, I will now compare Bulk API inserts with JDBC Template 400-person inserts per statement.\nMethod name Batch size Median duration (ms) Bulk Api 10000 3188.5 Jdbc Template 100 4175 Jdbc Template 10 4275 Jdbc Template 1000 4348 Bulk Api 1000 4358 Jdbc Template 10000 4697 Bulk Api 100 14440.5 Bulk Api 10 101251.5 Based on the results, we can see that the performance difference is much smaller than before. As the batch size increases, Bulk API performance improves. However, when we increase the batch size for JDBC Template to more than 100, the performance worsens. Therefore, we should be careful when setting the batch size for JDBC Template with multi-value inserts.\nConclusion Based on the results above, we can conclude that multi-value inserts drastically improve insert performances and that you should definitely consider it if you have performance problems. On the other hand, if you need to insert a huge amount of data, then Bulk API insert is the king.\nFor updates, you can follow me on Twitter or LinkedIn.\nThank you for your attention.\n","link":"https://www.professionaldev.pro/post/java/mssqlserver/fast_insert_part3/","section":"post","tags":["java","ms sql server"],"title":"Fastest Way To Insert the Data in MS SQL – Part 3 – Multi-value insert vs Bulk Api"},{"body":"Introduction In the previous post Fastest way to insert the data in MS SQL - Part 1 Hibernate Batching we compared the performance of Hibernate inserts with batching turned off vs batching turned on. In this post, we will explore other ways to insert data into MS SQL server and compare performance with Hibernate.\nWe will consider four different ways to persist data:\nUsing Hibernate batching, calling persist method. Manually inserting data using a prepared statement with batching. Using Spring Data Jdbc Template with batching. Using Microsoft Bulk API. Hibernate Batching Hibernate batching is implemented as following:\n1@Override 2public \u0026lt;T\u0026gt; void persistAndFlushInChunks(List\u0026lt;T\u0026gt; objects, int bulkInsertBatchSize) { 3\tif(CollectionUtils.isEmpty(objects)){ 4\treturn; 5\t} 6 7\t// set manual flush mode 8\tentityManager.setFlushMode(FlushModeType.COMMIT); 9\tentityManager 10\t.unwrap(Session.class) 11\t.setJdbcBatchSize(bulkInsertBatchSize); 12 13\tfor(List\u0026lt;T\u0026gt; chunk : Lists.partition(objects, bulkInsertBatchSize)) { 14\tpersistAndFlushObjects(chunk); 15\t} 16} 17 18private \u0026lt;T\u0026gt; void persistAndFlushObjects(Iterable\u0026lt;T\u0026gt; objects) { 19\tfor (T object : objects) { 20\tentityManager.persist(object); 21\t} 22\tentityManager.flush(); 23\tentityManager.clear(); 24} This post details the identical implementation as the previous one. I enabled manual flush mode to gain enhanced control over when data gets transmitted to the MS SQL server. Following each flush, I clear the entity manager, as the objects have already been persisted in the database and are no longer needed in memory.\nManually Inserting Data With Prepared Statement Since all actions are performed manually, a substantial amount of code has been written to facilitate this process. Therefore, I will simply outline the key components and provide an explanation of how each element operates.\n1public \u0026lt;T\u0026gt; void insert(List\u0026lt;? extends T\u0026gt; objects, TableDescriptor\u0026lt;T\u0026gt; descriptor, int batchSize) { 2 Session session = entityManager.unwrap(Session.class); 3 4 session.doWork(connection -\u0026gt; 5 insertInternal(objects, descriptor, connection, batchSize)); 6} 7 8private \u0026lt;T\u0026gt; void insertInternal(List\u0026lt;? extends T\u0026gt; objects, 9 TableDescriptor\u0026lt;T\u0026gt; descriptor, 10 Connection connection, 11 int batchSize) throws SQLException { 12 13 try (PreparedStatement preparedStatement = prepareInsertStatement(descriptor, connection, 1)) { 14 15 for (int i = 0; i \u0026lt; objects.size(); i++) { 16 @NonNull List\u0026lt;ColumnDescriptor\u0026lt;T\u0026gt;\u0026gt; columns = descriptor.getColumns(); 17 for (int j = 0; j \u0026lt; columns.size(); j++) { 18 ColumnDescriptor\u0026lt;T\u0026gt; columnDescriptor = columns.get(j); 19 preparedStatement.setObject(j + 1, columnDescriptor.getObjectFieldValue(objects.get(i))); 20 } 21 preparedStatement.addBatch(); 22 if ((i + 1) % batchSize == 0) { 23 preparedStatement.executeBatch(); 24 preparedStatement.clearBatch(); 25 } 26 } 27 preparedStatement.executeBatch(); 28 preparedStatement.clearBatch(); 29 } 30} The method takes three parameters: a list of Person objects to insert, a TableDescriptor providing information about the database table, and a batch size. Table descriptor contains column names and a mapping function for data that is used for preparing statement: columnDescritor.getObjectFieldValue(object).\nThis method utilizes Hibernate to obtain a database session and connection. The actual insertion process is carried out by the insertInternal method, which prepares a PreparedStatement for inserting data. It iterates through the list of objects, sets their values in the prepared statement, and adds them to a batch. When the specified batch size is reached, the batch is executed, and this process is repeated until all objects are inserted.\nUsing Spring Data Jdbc Template With Batching 1public \u0026lt;T\u0026gt; void insertBulk(TableDescriptor\u0026lt;T\u0026gt; tableDescriptor, 2 List\u0026lt;? extends T\u0026gt; data, 3 int batchSize) { 4 if (data.isEmpty()) return; 5 String insertQuery = getInsertQuery(tableDescriptor, 1); 6 jdbcTemplate.batchUpdate(insertQuery, 7 data, 8 batchSize, 9 (ps, argument) -\u0026gt; { 10 List\u0026lt;ColumnDescriptor\u0026lt;T\u0026gt;\u0026gt; columns = tableDescriptor.getColumns(); 11 for (int i = 0; i \u0026lt; columns.size(); i++) { 12 ColumnDescriptor\u0026lt;T\u0026gt; columnDescriptor = columns.get(i); 13 ps.setObject(i + 1, columnDescriptor.getObjectFieldValue(argument)); 14 } 15 }); 16} This method represents the most straightforward approach I've employed for batch data persistence. It leverages the Spring data JdbcTemplate class, which offers a user-friendly interface and abstracts away the intricacies of batching within the method. I supplied the query, data, and batchSize as parameters, with the only manual task being the population of a prepared statement with the data.\nMicrosoft Bulk Api 1public \u0026lt;T\u0026gt; void performBulkInsert(List\u0026lt;? extends T\u0026gt; data, 2 TableDescriptor\u0026lt;T\u0026gt; descriptor, 3 int batchSize) { 4 Session unwrap = entityManager.unwrap(Session.class); 5 6 unwrap.doWork((connection) -\u0026gt; { 7 try (SQLServerBulkCopy bulkCopy = 8 new SQLServerBulkCopy(connection.unwrap(SQLServerConnection.class))) { 9 SQLServerBulkCopyOptions options = new SQLServerBulkCopyOptions(); 10 options.setBatchSize(batchSize); 11 bulkCopy.setBulkCopyOptions(options); 12 bulkCopy.setDestinationTableName(descriptor.getTableName()); 13 14 CachedRowSet dataToInsert = createCachedRowSet(data, descriptor); 15 // Perform bulk insert 16 bulkCopy.writeToServer(dataToInsert); 17 } 18 }); 19} 20 21private \u0026lt;T\u0026gt; CachedRowSet createCachedRowSet(List\u0026lt;? extends T\u0026gt; data, TableDescriptor\u0026lt;T\u0026gt; descriptor) throws SQLException { 22 RowSetFactory factory = RowSetProvider.newFactory(); 23 CachedRowSet rowSet = factory.createCachedRowSet(); 24 25 rowSet.setMetaData(createMetadata(descriptor)); 26 27 for (T rowData : data) { 28 rowSet.moveToInsertRow(); 29 for (int i = 0; i \u0026lt; descriptor.getColumns().size(); i++) { 30 ColumnDescriptor\u0026lt;T\u0026gt; column = descriptor.getColumns().get(i); 31 rowSet.updateObject(i + 1, column.getObjectFieldValue(rowData), column.getSqlType()); 32 } 33 rowSet.insertRow(); 34 } 35 36 rowSet.moveToCurrentRow(); 37 return rowSet; 38} 39 40private static \u0026lt;T\u0026gt; RowSetMetaData createMetadata(TableDescriptor\u0026lt;T\u0026gt; descriptor) throws SQLException { 41 RowSetMetaData metadata = new RowSetMetaDataImpl(); 42 43 // Set the number of columns 44 metadata.setColumnCount(descriptor.getColumns().size()); 45 for (int i = 0; i \u0026lt; descriptor.getColumns().size(); i++) { 46 metadata.setColumnName(i + 1, descriptor.getColumns().get(i).getColumnName()); 47 metadata.setColumnType(i + 1, descriptor.getColumns().get(i).getSqlType()); 48 } 49 return metadata; 50} This data insertion method operates in much the same way as the preceding ones. The primary distinction lies in its utilization of the MS SQL server bulk API. This API is typically employed for importing data from a file into a table. However, in this instance, the source is a collection held in memory, rather than a file.\nResults I conduct the tests using varying batch sizes for each method, specifically batch sizes of 10, 100, and 1,000. 100,000 Person objects are persisted to the database.\nIn each test scenario, I run the test 10 times and compute the median value from the results. Following the completion of each test, I truncate the Person table. My measurements exclusively focus on the time taken for the insert operations to conclude.\nMethod Name Median Duration Ms Batch Size Bulk Api 314 10000 Bulk Api 432 1000 Bulk Api 1504 100 Jdbc Template 1508.5 10000 Prepared Statement 1520 10000 Jdbc Template 1551.5 1000 Prepared Statement 1559.5 1000 Hibernate Persist 1732 10000 Prepared Statement 1770.5 100 Hibernate Persist 1778 1000 Jdbc Template 1783 100 Hibernate Persist 2281 100 Prepared Statement 2583 10 Jdbc Template 2618.5 10 Hibernate Persist 4850.5 10 Bulk Api 10178.5 10 Conclusion Right from the start, it's apparent that the Bulk API is both the speediest and, paradoxically, the slowest data insertion method. When dealing with small batch sizes, its performance can be agonizingly sluggish. However, as we scale up the batch size, it truly shines and demonstrates itself as the optimal and swiftest means of inserting data into the MS SQL server database.\nThe Jdbc Template and Prepared Statement methods exhibit nearly identical performance levels, which isn't surprising since they essentially share the same underlying code.\nFurthermore, we can deduce that Hibernate Persist is the least efficient method for data persistence across various batch sizes (excluding a batch size of 10). While the difference in speed is relatively minor for larger batch sizes, it introduces approximately a 10% increase in processing time for batch sizes of 10,000 and 1,000. Moreover, this overhead becomes more pronounced when dealing with smaller batch sizes.\nWe can employ an additional technique when working with a prepared statement. In our previous tests, we conducted inserts using the following statement:\n1insert into Person(person_id, user_name, first_name, last_name, years) values (?,?,?,?,?) With this statement, we insert one person object at a time. However, MS SQL Server supports an alternative syntax:\n1insert into Person(person_id, user_name, first_name, last_name, years) values (?,?,?,?,?),(?,?,?,?,?)... Allowing us to insert multiple sets of values in a single statement, up to a limit of 2,100 parameters. In my upcoming post, I will assess the performance of this method and compare it to the fastest Bulk API insert to determine its efficiency.\nFor updates, you can follow me on Twitter or LinkedIn.\nThank you for your attention.\n","link":"https://www.professionaldev.pro/post/java/mssqlserver/fast_insert_part2/","section":"post","tags":["java","ms sql server"],"title":"Fastest Way To Insert the Data in MS SQL – Part 2 – Hibernate vs Prepared Statement vs Jdbc Template vs Bulk Api"},{"body":"Introduction Currently, my team is focused on optimizing the performance of our web services. Our data manipulation tasks primarily rely on Hibernate for interacting with our MS SQL Server database. Hibernate provides a convenient and straightforward API for handling data, particularly for persisting individual or interconnected objects (object graph). However, when it comes to bulk operations, Hibernate's default behavior may not be as efficient. To address this, you can enhance performance by enabling Hibernate batching in the application.properties file.\n1hibernate.jdbc.batch_size=\u0026lt;batch size\u0026gt; You have the option to dynamically adjust the batch size by manipulating the Session object. To illustrate, if you intend to configure a batch size of 50 for a specific session, you can achieve this goal with the following code snippet:\n1entityManager 2 .unwrap(Session.class) 3 .setJdbcBatchSize(batchSize); Once you have enabled Hibernate batching, you can start to see performance improvements in your application, especially if you are performing a large number of database operations.\nHere are some additional tips for using Hibernate batching: A batch size that is too small will not provide much performance improvement, while a batch size that is too large can cause memory problems. To enable Hibernate to batch all statements, you need to set the hibernate.order_inserts and hibernate.order_updates properties to true. If you work wit h Spring Boot application, you can enable batching by using prefix spring.jpa.properties.hibernate.*. Example Let's try to persist list of Person objects using Hibernate. Person object is defined as following:\n1@AllArgsConstructor 2@NoArgsConstructor 3@Entity 4@Table(name = PERSON_TABLE_NAME) 5@Getter 6@Setter 7public final class Person { 8 @Id 9 @Column(name = \u0026#34;person_id\u0026#34;, columnDefinition = \u0026#34;INT\u0026#34;) 10 private int personId; 11 @Column(name = \u0026#34;user_name\u0026#34;, nullable = false, length = 30, columnDefinition = \u0026#34;NVARCHAR(30)\u0026#34;) 12 private String userName; 13 @Column(name = \u0026#34;first_name\u0026#34;, nullable = false, length = 10, columnDefinition = \u0026#34;NVARCHAR(10)\u0026#34;) 14 private String firstName; 15 @Column(name = \u0026#34;last_name\u0026#34;, nullable = false, length = 15, columnDefinition = \u0026#34;NVARCHAR(15)\u0026#34;) 16 private String lastName; 17 @Column(name = \u0026#34;years\u0026#34;, columnDefinition = \u0026#34;INT\u0026#34;) 18 private int years; 19} As you can tell, this is a pretty standard Hibernate entity with 2 integer fields and 3 string fields. Just remember, the personId doesn't get created on its own; you've got to give it a value.\nRandom data is generated using following method;\n1private static Person genRandomPerson() { 2 ++personId; 3 return new Person(personId, 4 RandomStringUtils.randomAlphanumeric(30), 5 RandomStringUtils.randomAlphabetic(10), 6 RandomStringUtils.randomAlphabetic(15), 7 RandomUtils.nextInt(10, 100) 8 ); 9} We will consider 2 different ways to insert this data.\nUsing persist method from EntityManager. Using saveAll method from PersonRepository. Those functions are defined as following:\n1public void persist(List\u0026lt;Person\u0026gt; people, int batchSize) { 2 if(CollectionUtils.isEmpty(people)){ 3 return; 4 } 5 6 // set manual flush mode 7 entityManager.setFlushMode(FlushModeType.COMMIT); 8 // flush and clear everything from entity manager 9 entityManager.flush(); 10 entityManager.clear(); 11 12 for(List\u0026lt;Person\u0026gt; chunk : Lists.partition(people, batchSize)) { 13 persistAndFlushObjects(chunk); 14 } 15 entityManager.setFlushMode(FlushModeType.AUTO); 16} 17 18private \u0026lt;T\u0026gt; void persistAndFlushObjects(Iterable\u0026lt;T\u0026gt; objects) { 19 for (T object : objects) { 20 entityManager.persist(object); 21 } 22 entityManager.flush(); 23 entityManager.clear(); 24} The persist function is the one that uses the entityManager to save objects. It's crucial to emphasize that this function should exclusively be used for inserting new objects into the database. It accomplishes this by partitioning the input list into multiple sublists, each of which has a size batchSize. Subsequently, after each sublist is persisted into the entity manager, the flush and clear methods are invoked to both transmit the data to the database and reset the persistence context.\nIn addition, we set flush mode to manual, all changes are deliberately flushed to the database before initiating the process. This approach allows for more precise control over when the data is actually transmitted to the database.\nAnd here is the second function:\n1public void saveAll(List\u0026lt;Person\u0026gt; people) { 2 personRepository.saveAllAndFlush(people); 3} This function simply uses PersonRepository method saveAllAndFlush. Person repository is standard JPA repository defined as following:\n1@Repository 2public interface PersonRepository extends JpaRepository\u0026lt;Person, Integer\u0026gt; {} Results I execute these two methods in distinct scenarios: one with the batching parameter turned on and the other with it turned off. Additionally, I conduct the tests using varying batch sizes for each scenario, specifically batch sizes of 10, 100, and 1,000. 100,000 Person objects are persisted to the database.\nIn each test scenario, I run the test 10 times and compute the median value from the results. Following the completion of each test, I truncate the Person table. My measurements exclusively focus on the time taken for the insert operations to conclude.\nHere are the results:\nMethod name Median duration ms Batch size PersistBatched 1675 1000 PersistBatched 2237.5 100 PersistBatched 4787 10 Persist 10767.5 10 Persist 10797.5 100 Persist 11085 1000 SaveAllBatched 12756.5 1000 SaveAllBatched 12993 100 SaveAllBatched 14059 10 SaveAll 21133 1000 SaveAll 21164.5 100 SaveAll 21908.5 10 Indeed, the chart clearly demonstrates that enabling batching results in the fastest data insertion. It's also evident that larger batch sizes correspond to faster persistence. However, it's essential to exercise caution when considering further increases in batch size, as this can lead to memory usage issues.\nThe unexpected second-place performance is intriguing, given the conventional expectation that batched methods should outperform. To gain a deeper understanding of what's happening, we'll employ a profiler and examine the SQL operations. Specifically, I will activate the profiler and insert at least 3 persons with a batch size of 3 for each method to investigate this further.\nPersist - batching off 1exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;FFZQXDJQUW\u0026#39;,N\u0026#39;CtJFyhZDmQObnmk\u0026#39;,N\u0026#39;aTUyaBJdYD1sZJMshs8sRPIVLg4rKs\u0026#39;,39,1 2exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;BjFjOuBuJH\u0026#39;,N\u0026#39;yYdQwHJXsJfIqlM\u0026#39;,N\u0026#39;qIaa3HcHb2uJqJe6uey8j6Ia7wQtO4\u0026#39;,69,2 3exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;ssXhcZASed\u0026#39;,N\u0026#39;AQGJYshznwUtSVs\u0026#39;,N\u0026#39;YysiILjKLdzoneD0jgh2FL37QIiRpI\u0026#39;,47,3 SaveAll - batching off 1 2exec sp_executesql N\u0026#39;select p1_0.person_id,p1_0.first_name,p1_0.last_name,p1_0.user_name,p1_0.years from person p1_0 where p1_0.person_id= @P0 \u0026#39;,N\u0026#39;@P0 int\u0026#39;,7 3exec sp_executesql N\u0026#39;select p1_0.person_id,p1_0.first_name,p1_0.last_name,p1_0.user_name,p1_0.years from person p1_0 where p1_0.person_id= @P0 \u0026#39;,N\u0026#39;@P0 int\u0026#39;,8 4exec sp_executesql N\u0026#39;select p1_0.person_id,p1_0.first_name,p1_0.last_name,p1_0.user_name,p1_0.years from person p1_0 where p1_0.person_id= @P0 \u0026#39;,N\u0026#39;@P0 int\u0026#39;,9 5exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;jetpalOgkj\u0026#39;,N\u0026#39;MnSwdgaGgvSSRkr\u0026#39;,N\u0026#39;KBEOdFrt8b2D4EXP1waY5YtBWIvNKK\u0026#39;,48,7 6exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4int\u0026#39;,N\u0026#39;ZxhPKruIKE\u0026#39;,N\u0026#39;wsupRsfIfmCypry\u0026#39;,N\u0026#39;Y8mRpSzqd0RzZhJ9zLT07XelNtM8L0\u0026#39;,24,8 7exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;HKFDsDrUON\u0026#39;,N\u0026#39;wubsXMqCnSWoIIp\u0026#39;,N\u0026#39;Q2POp78YTUeFr1W2G3XwB7H9EkgpKH\u0026#39;,42,9 SaveAll - batching on 1 2exec sp_executesql N\u0026#39;select p1_0.person_id,p1_0.first_name,p1_0.last_name,p1_0.user_name,p1_0.years from person p1_0 where p1_0.person_id= @P0 \u0026#39;,N\u0026#39;@P0 int\u0026#39;,4 3exec sp_executesql N\u0026#39;select p1_0.person_id,p1_0.first_name,p1_0.last_name,p1_0.user_name,p1_0.years from person p1_0 where p1_0.person_id= @P0 \u0026#39;,N\u0026#39;@P0 int\u0026#39;,5 4exec sp_executesql N\u0026#39;select p1_0.person_id,p1_0.first_name,p1_0.last_name,p1_0.user_name,p1_0.years from person p1_0 where p1_0.person_id= @P0 \u0026#39;,N\u0026#39;@P0 int\u0026#39;,6 5exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;UKXIAjQpKW\u0026#39;,N\u0026#39;JIbTaOjCPMyyWnl\u0026#39;,N\u0026#39;bVISG7MpaJriTL1tjwYtE5pPKtSEKI\u0026#39;,96,4 6declare @p1 int 7set @p1=1 8exec sp_prepexec @p1 output,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;NiZUusgbTI\u0026#39;,N\u0026#39;QOtzVxVYnDVTiml\u0026#39;,N\u0026#39;WEjQzTHKqwPco0TcYA7tiDLpC0neWk\u0026#39;,95,5 9select @p1 10exec sp_execute 1,N\u0026#39;NnKjQPqXam\u0026#39;,N\u0026#39;QCgmsEFKwWdNRBV\u0026#39;,N\u0026#39;vu3NXw6KRZ3fZ3wc3Sx3Wk2dYlGFmg\u0026#39;,71,6 Persist - batching off 1 2exec sp_executesql N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;HgTNGeptBc\u0026#39;,N\u0026#39;YvYmAUhiAfnQRTx\u0026#39;,N\u0026#39;E2oyRByYVlaqWs6xamqmHtEl2TUp2J\u0026#39;,43,10 3declare @p1 int 4set @p1=2 5exec sp_prepexec @p1 output,N\u0026#39;@P0 nvarchar(4000),@P1 nvarchar(4000),@P2 nvarchar(4000),@P3 int,@P4 int\u0026#39;,N\u0026#39;insert into person (first_name,last_name,user_name,years,person_id) values ( @P0 , @P1 , @P2 , @P3 , @P4 )\u0026#39;,N\u0026#39;WHlTNmwAhz\u0026#39;,N\u0026#39;MCUejiZXmAaQayn\u0026#39;,N\u0026#39;yJNPUG9ejuOBiFyz3koKVSqYtvA527\u0026#39;,73,11 6select @p1 7exec sp_execute 2,N\u0026#39;gDiAwqlEtU\u0026#39;,N\u0026#39;oJllYKcGNkXpxie\u0026#39;,N\u0026#39;AyKsziLzNYdFj4QHSoPcVXwEiCuOKF\u0026#39;,21,12 The observed behavior where select statements are executed for each insert statement when the saveAll method is invoked is due to a specific reason. When we use the save method with supplied identity, Hibernate cannot be certain whether the record already exists in the database. Consequently, it needs to verify this by executing a select statement for each entity, leading to the additional select operations during the insertion process.\nHere is the code snippet that defines the save method within the Hibernate source code:\n1@Transactional 2@Override 3public \u0026lt;S extends T\u0026gt; S save(S entity) { 4 5\tAssert.notNull(entity, \u0026#34;Entity must not be null\u0026#34;); 6 7\tif (entityInformation.isNew(entity)) { 8\tem.persist(entity); 9\treturn entity; 10\t} else { 11\treturn em.merge(entity); 12\t} 13} Conclusion The results clearly indicate that enabling the batching parameter significantly enhances insert performance when working with Hibernate. Therefore, considering the activation of batching in your project can yield substantial performance improvements.\nThe second key takeaway is the importance of exercising caution and continuously monitoring performance when working with Hibernate. It's possible that Hibernate may not operate optimally out of the box, but with some straightforward code adjustments, you can achieve dramatic performance enhancements. This highlights how important it is to carefully measure performance and examine it closely to get the best results.\nFor updates, you can follow me on Twitter or LinkedIn.\n","link":"https://www.professionaldev.pro/post/java/mssqlserver/persist_vs_saveall/","section":"post","tags":["java","ms sql server"],"title":"Fastest way to insert the data in MS SQL - Part 1 Hibernate Batching"},{"body":"","link":"https://www.professionaldev.pro/tags/hashmap/","section":"tags","tags":null,"title":"hashMap"},{"body":"Introduction In a previous article, I discussed how to efficiently store strings in a hash map and search for them without worrying about case sensitivity. I explained how creating a custom wrapper class for String and overriding its hashCode and equals methods can achieve this goal, while ensuring good performance by minimizing the creation of additional strings.\nHowever, this approach may not be ideal if you need to store objects that contain multiple strings. While you could still use the same wrapper class, there are at least two potential downsides to this approach. Firstly, it would result in increased memory usage due to the creation of additional objects. Secondly, there may be situations where you are unable or unwilling to modify the String type in carrier classes.\nIn this article, I will explore alternative solutions to this problem that can improve memory usage and avoid the need to modify existing classes.\nExample Suppose we have a Person class with firstName and lastName fields. In order to ignore casing when comparing instances of this class, we can override the equals and hashCode methods.\nThe easiest way to implement these methods is to use an IDE, such as IntelliJ, which can automatically generate the code for you in a generic way. The code generator can be accessed through the IDE's interface, as shown below:\nBy default, the IDE will generate code using the Objects.equals method, which performs a case-sensitive comparison. The generated code with default methods can be seen below:\n1public record Person (String firstName, String lastName) { 2 @Override 3 public boolean equals(Object o) { 4 if (this == o) return true; 5 if (o == null || getClass() != o.getClass()) return false; 6 7 Person person = (Person) o; 8 9 if (!Objects.equals(firstName, person.firstName)) return false; 10 return Objects.equals(lastName, person.lastName); 11 } 12 13 @Override 14 public int hashCode() { 15 int result = firstName != null ? firstName.hashCode() : 0; 16 result = 31 * result + (lastName != null ? lastName.hashCode() : 0); 17 return result; 18 } 19} We can modify the equals and hashCode methods to use our custom comparison method instead, as shown in the code below:\n1public record Person (String firstName, String lastName) { 2 3 @Override 4 public boolean equals(Object o) { 5 if (this == o) return true; 6 if (o == null || getClass() != o.getClass()) return false; 7 8 Person person = (Person) o; 9 10 if (!StringUtils.ciEquals(firstName, person.firstName)) return false; 11 return StringUtils.ciEquals(lastName, person.lastName); 12 } 13 14 @Override 15 public int hashCode() { 16 int result = StringUtils.ciHashCode(firstName); 17 result = 31 * result + StringUtils.ciHashCode(lastName); 18 return result; 19 } 20} Note: In the examples, records are used instead of classes. Records are essentially the same as classes, but they require less boilerplate code to create. This makes them ideal for demonstrating examples and reducing clutter in the code.\nRather than implementing a custom comparison method directly in a class, it is generally considered a better practice to place it in a utility class, such as StringUtils. This allows the method to be used across multiple classes and methods, and it also helps to keep the code modular and reusable. By implementing the custom comparison method in a utility class, we can easily ignore case sensitivity when comparing strings without having to create big changes.\nLet's check how StringUtils class is implemented:\n1public class StringUtils { 2 private StringUtils() {} 3 4 public static int ciHashCode(String stringVal) { 5 if (stringVal == null) { 6 return 0; 7 } 8 int h = 0; 9 if (stringVal.length() \u0026gt; 0) { 10 for (int i = 0; i \u0026lt; stringVal.length(); i++) { 11 h = 31 * h + Character.toLowerCase(stringVal.charAt(i)); 12 } 13 } 14 return h; 15 } 16 17 public static boolean ciEquals(String a, String b) { 18 return a == b || a != null \u0026amp;\u0026amp; a.equalsIgnoreCase(b); 19 } 20} It's worth noting that if you are using the Apache Commons library in your project, you don't need to create a custom method for comparing strings, as the library already provides a method equalsIgnoreCase which does the same thing and it is located in the class with the same name.\nThe hashCode function is implemented similarly to the hashCode function in the String class. There are two differences. First, the value of the hash code is not stored in a variable for caching. Second, it converts every character to lowercase before using it for calculation. This allows us to efficiently store objects in a hash map without considering case sensitivity and by preserving original casing. By using these simple functions and an IDE code generator, we can create a class that is efficient for searching by keys.\nConclusion Using these simple functions and an IDE code generator, you can create case-insensitive classes that can be efficiently stored in a HashMap or other collections. This is particularly useful for storing objects as keys, as HashMaps provide fast key-based searches.\nIf you liked the content, you can find me on Twitter at @mare_milenkovic and on LinkedIn at mare-milenkovic.\n","link":"https://www.professionaldev.pro/post/java/case_insenstive_hash_code/","section":"post","tags":["java","hashMap","strings"],"title":"How to Write Classes With Multiple Case Insensitive Strings"},{"body":"","link":"https://www.professionaldev.pro/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://www.professionaldev.pro/tags/strings/","section":"tags","tags":null,"title":"strings"},{"body":"Introduction By default, Microsoft SQL Server processes strings without considering their case sensitivity. Java, unlike Microsoft SQL Server, is case-sensitive which can result in problems. Specifically, on the project I'm working on, there have been numerous bugs caused by the mismatch in case sensitivity.\nGenerally, there are many situations where case-insensitive strings are necessary. One such example is with email addresses, which are inherently case-insensitive. Therefore, the issue of case sensitivity is not solely related to the mismatch between MS SQL Server and Java, but rather it is a more widespread concern.\nIn the following, I will explain how case sensitivity can cause issues in code, and how and why using a case-insensitive hash map can provide a solution.\nExample Let's say:\nThe groups API provides a service called GroupsService which accepts a list of user email addresses and creates a group with those users. Each user's email address is unique in the EmailGroups database. Another API called users is used to create new users with their email addresses and store them in a database. When a request is made to the GroupsService service, each email address in the list is validated by checking a database using a case-insensitive search. If an email address in the list is not found in the database, an error is thrown and the error message includes the exact email address that wasn't found. The GroupsService service is designed to handle large requests, so it employs a solution that checks email addresses in chunks, rather than one-by-one against the database. Implementation Retrieve emails (along with any other data necessary for processing the request) from the database by filtering them using the email addresses provided in the incoming request. Store the retrieved results in a Java HashMap. Iterate through each email address in the incoming request and check if there is a corresponding entry in the HashMap. If an entry doesn't exist for an email address, an error is thrown to the user. We can use the following Java code to implement described behaviour:\n1public class GroupsService { 2 @Value 3 private static class MemberInfo { 4 int id; 5 String email; 6 String name; 7 } 8 public void createGroupWithMembership(@NonNull String groupName, @NonNull List\u0026lt;String\u0026gt; memberEmails) { 9 Map\u0026lt;String, MemberInfo\u0026gt; stringMemberInfoMap = loadMemberInfoByEmails(memberEmails); 10 11 for (String memberEmail : memberEmails) { 12 if (!stringMemberInfoMap.containsKey(memberEmail)) { 13 throw new IllegalArgumentException(\u0026#34;Can\u0026#39;t find email: \u0026#39;%s\u0026#39; in the system.\u0026#34;); 14 } 15 } 16 17 } 18 19 private Map\u0026lt;String, MemberInfo\u0026gt; loadMemberInfoByEmails(List\u0026lt;String\u0026gt; memberEmails) { 20 List\u0026lt;String\u0026gt; memberEmailsToLookupFor = memberEmails.stream().distinct().sorted().toList(); 21 List\u0026lt;MemberInfo\u0026gt; memberInfoList = groupRepository.loadMemberInfoByEmails(memberEmailsToLookupFor); 22 return 23 memberInfoList 24 .stream() 25 .collect(Collectors.toMap(MemberInfo::getEmail, Function.identity())); 26 } 27} The Problem Since the database is case-insensitive, it will return all emails without considering the case of the input string. However, in Java, we use a HashMap to match incoming emails with emails from the database. The issue with this approach is that HashMap keys are case-sensitive, which means that some emails may not be found.\nTo illustrate the problem, consider the following example:\n1 INPUT LIST HASH MAP 2 (lower case emails) (case-sensitive email keys) 3 4 +------------------------+ +------------------------+ 5 | alice@example.com | | alice@example.com | MemberInfo 6 +------------------------+ +------------------------+ 7 | BOB@example.com (*) | | Bob@example.com | MemberInfo 8 +------------------------+ +------------------------+ 9 | charlie@example.com | | charlie@example.com | MemberInfo 10 +------------------------+ +------------------------+ 11 | DAVID@example.com (*) | | David@example.com | MemberInfo 12 +------------------------+ +------------------------+ Sure, here's a clearer rewrite of the text:\nIn the previous example, Bob and David have different casing in their email addresses. Because the hashCode function generates different hash codes for the same string with different casing, the hash map will not be able to find their email addresses in the database. This can lead to errors for the user, even though we have data about both email addresses in the database.\nSolution 1: Use TreeMap The solution to the problem of case-sensitive hash maps in Java is to use a TreeMap instead of a HashMap, as it can accept a custom comparator. By using a case-insensitive comparator, the TreeMap will be able to match email addresses with different casing, and the correct MemberInfo can be retrieved from the database. Here is an example of how to use a TreeMap with a custom comparator in Java:\n1Map\u0026lt;String, String\u0026gt; mapping = new TreeMap(String::caseInsensitiveComparator); While TreeMap offers good performance, it may not be as fast as HashMap for large datasets. The query time increases with the size of the dataset. However, searching the data using TreeMap is similar to performing a binary search, so the access time doesn't drop linearly. Although TreeMap provides good performance, it can't match the nearly constant O(1) access time of HashMap.\nSolution 2: Use HashMap Unfortunately, hash maps only work based on object hashCode and equals methods, and it doesn't allow for custom functions to be supplied at the constructor or in any other way. This can be limiting, and as a result, many programmers opt for TreeMap.\nHowever, as professional developers, we can create a better implementation for case-insensitive string equals and hash code methods. There are two main approaches:\nConvert all strings to lower/upper case. However, this creates a lot of new objects, which doubles memory usage, and we always need to lower/upper case input strings for the hash map. This can create memory churn in high-load scenarios, and it is also required to store strings with the original casing, so we need a way to map from lower/upper cased strings to the original ones. Due to all these challenges, this approach is not recommended.\nExtend the String class and override the hashCode and equals methods. Alternatively, we can create a new type, store the string as a final local variable, and implement custom equals and hashCode methods. This approach still requires the creation of new objects, but compared to creating new strings, memory allocation is much lower.\nThe challenge now is to implement those two methods. For equals, we can use the equalsIgnoreCase method instead of toLowerCase, which avoids the problems mentioned above. For the hashMethod, we can copy the logic from the String class and use the lowercase version of the characters for generating the hashCode. Here is an implementation example:\n1@RequiredArgsConstructor(staticName = \u0026#34;of\u0026#34;) 2public class CiString { 3 @NonNull 4 private final String stringVal; 5 private int hash = 0; 6 7 @Override 8 public boolean equals(Object o) { 9 if (this == o) return true; 10 11 if (o == null || getClass() != o.getClass()) return false; 12 CiString ciString = (CiString) o; 13 return stringVal.equalsIgnoreCase(ciString.toString()); 14 } 15 public int hashCode() { 16 int h = hash; 17 if (h == 0 \u0026amp;\u0026amp; stringVal.length() \u0026gt; 0) { 18 for (int i = 0; i \u0026lt; stringVal.length(); i++) { 19 h = 31 * h + Character.toLowerCase(stringVal.charAt(i)); 20 } 21 hash = h; 22 } 23 return h; 24 } 25 @Override 26 public String toString() { 27 return stringVal; 28 } 29} Furthermore, if our intention is to use the CiString class only as keys in a hash map, and we calculate the hash code only once, we can remove the private int hash property from the class. This optimization can conserve memory space since the hash code is only necessary for determining the bucket location in the hash map, and not for any other tasks.\nAs professional developers, it's important to write tests to ensure the correctness and reliability of our code. Here is an example of a test class for the case-insensitive string hash map implementation:\n1public class CiStringTest { 2 @Test 3 public void nvl() { 4 Assertions.assertNull(CiString.nvl(null)); 5 Assertions.assertNotNull(CiString.nvl(\u0026#34;\u0026#34;)); 6 } 7 @Test 8 public void testEquals() { 9 Assertions.assertEquals(CiString.of(\u0026#34;String\u0026#34;), CiString.of(\u0026#34;sTRING\u0026#34;)); 10 Assertions.assertNotEquals(null, CiString.of(\u0026#34;\u0026#34;)); 11 } 12 @Test 13 public void testHashCode_caseInsensitive() { 14 CiString lowerCase = CiString.of(\u0026#34;sTriNg\u0026#34;); 15 CiString upperCase = CiString.of(\u0026#34;StRING\u0026#34;); 16 Assertions.assertEquals(lowerCase.hashCode(), upperCase.hashCode()); 17 } 18 @Test 19 public void of_whenNullString_thenException() { 20 NullPointerException nullPointerException = Assertions 21 .assertThrows(NullPointerException.class, () -\u0026gt; CiString.of(null)); 22 23 Assertions.assertEquals(\u0026#34;stringVal is marked non-null but is null\u0026#34;, nullPointerException.getMessage()); 24 } 25 @Test 26 public void toStringTest() { 27 Assertions.assertEquals(\u0026#34;test\u0026#34;, CiString.of(\u0026#34;test\u0026#34;).toString()); 28 } 29} Other Solutions: Baeldung In this article, you can find a description of the problem we are discussing. The article also proposes several solutions to the problem, including the previously mentioned Solution 1. In addition to that, the article suggests using Apache's CaseInsensitiveMap or Spring's LinkedCaseInsensitiveMap as alternative solutions. However, it is important to note that both of these solutions are based on lowercasing the keys, which may not be the desired behavior in certain scenarios.\nI'm active on Twitter and LinkedIn, and I'd love it if you could give me a follow. You can find me on Twitter at @mare_milenkovic and on LinkedIn at mare-milenkovic.\n","link":"https://www.professionaldev.pro/post/java/case_insensitive_string_hashmap/","section":"post","tags":["java","hashMap","strings"],"title":"How to Use Case Insensitive String in Hash Map"},{"body":"","link":"https://www.professionaldev.pro/tags/peformance/","section":"tags","tags":null,"title":"peformance"},{"body":"","link":"https://www.professionaldev.pro/categories/peformance/","section":"categories","tags":null,"title":"peformance"},{"body":"Introduction Modern hardware is very good at predicting the next instructions to be executed. This kind of prediction allows CPUs to do more work in less time. One of the strategies that hardware relies on is data locality. This means that when CPU requests data from main memory, it not only retrieves the requested data, but the hardware also retrieves the data that is stored in proximity to the requested data.\nOn the software side, the Java compiler is heavily optimized to assist the hardware in these optimizations. Despite these optimizations, there are cases where none of them can help us. If we as programmers don't address this issue, it can lead to degraded application performance.\nBefore I continue, let me tell you this:\nDon't optimize your application prematurely!\nIf you don't have performance issues with your application, you don't need to optimize. However, it pays to know what can affect the performance of your application and how you can be more careful and write better performing code.\nWhat Is Memory Churn? Memory churn refers to the continuous and repetitive process of creating, deleting, and reallocating memory in a computer system. It is the rate at which memory is allocated and deallocated over a certain period of time, usually measured in cycles or seconds.\nMemory churn can have an impact on the overall performance of a computer system. When memory is constantly allocated and deallocated, it can lead to fragmentation of the memory space, which can result in slower performance and increased resource usage.\nOne common example of memory churn is in software applications that allocate and deallocate memory frequently, such as video games or web browsers. These applications can cause significant memory churn, leading to slower performance and increased resource usage over time. Overall, minimizing memory churn is important for maintaining optimal performance and resource usage in computer systems. This can be achieved through various strategies, such as efficient memory management and reducing unnecessary memory allocations and deallocations.\nThe JVM runs garbage collection periodically, either when it can, because the program threads are waiting for some external event, or when it needs to, because it's run out of memory for creating new objects. Despite the automatic nature of the process, it's important to understand that it's going on, because it can be a significant part of the overhead of Java programs.\nExample of Memory Churn One common example of memory churn that can occur in applications is when data is stored in a hash map with a composite key consisting of two combined strings. Let me show you an example:\nLet's say that we want to fetch a person data from in-memory Person repository. Contents of the PersonRepository and the Person class are defined as following:\n1@Value 2public class Person { 3 String firstName; 4 String lastName; 5 int age; 6} 7 8public class PersonRepository { 9 10 Map\u0026lt;String, Person\u0026gt; personNameMap = new HashMap\u0026lt;\u0026gt;(); 11 12 public Person findPersonByFullName(String firstName, String lastName) { 13 return personNameMap.get(firstName + lastName); 14 } 15 /*...*/ 16} Note: In the examples in this article, I used Lombok annotations to reduce the need for writing boilerplate code. You can find more details about it on the following link.\nOn first look, this code looks good to most of the developers. But, there is one performance issue that can arise if there is a massive load on the findPersonByFullName method. To understand the problem, let's check how the + operator for String concatenation works in Java. In Java 8, this concatenation is implemented using StringBuilder class. So the code for string concatenation would be implemented as follows:\n1public class Person { 2 /*...*/ 3 public Person findPersonByFullName(String firstName, String lastName) { 4 return personNameMap1.get(new StringBuilder().append(firstName).append(lastName).toString()); 5 } 6} 7 8public class StringBuilder extends AbstractStringBuilder { 9 /*...*/ 10 11 /** 12 * Constructs a string builder with no characters in it and an 13 * initial capacity of 16 characters. 14 */ 15 @IntrinsicCandidate 16 public StringBuilder() { 17 super(16); 18 } 19 /*...*/ 20} 21 22public class AbstractStringBuilder { 23 /*...*/ 24 /** 25 * Creates an AbstractStringBuilder of the specified capacity. 26 */ 27 AbstractStringBuilder(int capacity) { 28 if (COMPACT_STRINGS) { 29 value = new byte[capacity]; 30 coder = LATIN1; 31 } else { 32 value = StringUTF16.newBytesFor(capacity); 33 coder = UTF16; 34 } 35 } 36 /*...*/ 37} 38 39public class String /*...*/ { 40 /*...*/ 41 /* 42 * Package private constructor. Trailing Void argument is there for 43 * disambiguating it against other (public) constructors. 44 */ 45 String(AbstractStringBuilder asb, Void sig) { 46 byte[] val = asb.getValue(); 47 int length = asb.length(); 48 if (asb.isLatin1()) { 49 this.coder = LATIN1; 50 this.value = Arrays.copyOfRange(val, 0, length); 51 } else { 52 // only try to compress val if some characters were deleted. 53 if (COMPACT_STRINGS \u0026amp;\u0026amp; asb.maybeLatin1) { 54 byte[] buf = StringUTF16.compress(val, 0, length); 55 if (buf != null) { 56 this.coder = LATIN1; 57 this.value = buf; 58 return; 59 } 60 } 61 this.coder = UTF16; 62 this.value = Arrays.copyOfRange(val, 0, length \u0026lt;\u0026lt; 1); 63 } 64 } 65 /*...*/ 66} By examining the StringBuilder parameterless constructor, it becomes apparent that its character array is initialized with a default size of 16. If the concatenation's result exceeds this limit, a new array must be created and initialized. And all the data must be copied over to the new array. Finally, when the toString method is called, the array is once again copied to create the resulting string.\nThe + operator has much better implementation in Java 17. StringConcatFactory.makeConcatWithConstants method is used for strings concatenation. It further calls the method StringConcatHelper.simpleConcat for the special case of two strings concatenation. This method is the fastest implementation for the String concatenation that I have seen till now in Java. It is optimized to create just one character array for all the concatenations. And not only that, it uses the same character array to create a new instance of String class. This can be done only in jdk code, since the constructor for String class that accepts character array is package private. Here is the content of simpleConcat method.\n1/** JDK 18 code 2 * 3 * 4 * Perform a simple concatenation between two objects. Added for startup 5 * performance, but also demonstrates the code that would be emitted by 6 * {@code java.lang.invoke.StringConcatFactory$MethodHandleInlineCopyStrategy} 7 * for two Object arguments. 8 * 9 * @param first first argument 10 * @param second second argument 11 * @return String resulting string 12 */ 13@ForceInline 14static String simpleConcat(Object first, Object second) { 15 String s1 = stringOf(first); 16 String s2 = stringOf(second); 17 if (s1.isEmpty()) { 18 // newly created string required, see JLS 15.18.1 19 return new String(s2); 20 } 21 if (s2.isEmpty()) { 22 // newly created string required, see JLS 15.18.1 23 return new String(s1); 24 } 25 // start \u0026#34;mixing\u0026#34; in length and coder or arguments, order is not 26 // important 27 long indexCoder = mix(initialCoder(), s1); 28 indexCoder = mix(indexCoder, s2); 29 byte[] buf = newArray(indexCoder); 30 // prepend each argument in reverse order, since we prepending 31 // from the end of the byte array 32 indexCoder = prepend(indexCoder, buf, s2); 33 indexCoder = prepend(indexCoder, buf, s1); 34 return newString(buf, indexCoder); 35 } From the above description, it can be inferred that the example code generates a huge number of objects (specially in java 8 example). Garbage collector needs to clean up those objects at some point in time. The greater the number of memory allocations made by an application, the more the garbage collector is required to work in order to clean them up. Furthermore, these objects happen to be arrays. It means that objects are memory heavy. This can lead to additional strain on the garbage collector. As a result, memory churn may occur.\nHow Can We Fix It? The solution would be to reduce the number of allocations. For example, we can consider creating a pool of objects that can be reused (something like thread pools). The proposed solution wouldn't work on this example, because we don't know what instances of the String object will be created.\nWhat Can We Do Then? If we must allocate memory, then we should check if we can perform smaller memory allocations. We can create a new class that will contain references to existing strings. The class must implement equals and hashCode methods. Requested behaviour will be the same, but the amount of allocated memory will be significantly smaller. We can call this class PersonPk and implement it as follows:\n1@Value 2public class PersonPk { 3 private final String firstName; 4 private final String lastName; 5} By creating new objects with references to existing strings, we can reduce the burden on the garbage collector. This should improve performance.\nIn the end, I compared the performance of both methods and recorded the following results:\n1Java 8 2Benchmark (nameLength) Mode Cnt Score Error Units 3MemoryChurnBench.getFromRepoNewObject 5 thrpt 5 6366.565 ± 342.554 ops/s 4MemoryChurnBench.getFromRepoNewObject 10 thrpt 5 6365.775 ± 331.102 ops/s 5MemoryChurnBench.getFromRepoNewObject 100 thrpt 5 6388.885 ± 44.873 ops/s 6MemoryChurnBench.getFromRepoNewObject 1000 thrpt 5 6246.701 ± 531.240 ops/s 7MemoryChurnBench.getFromRepoNewObject 10000 thrpt 5 6564.168 ± 1421.583 ops/s 8MemoryChurnBench.getFromRepoStringConcat 5 thrpt 5 3151.173 ± 39.861 ops/s 9MemoryChurnBench.getFromRepoStringConcat 10 thrpt 5 2531.838 ± 89.890 ops/s 10MemoryChurnBench.getFromRepoStringConcat 100 thrpt 5 391.318 ± 0.190 ops/s 11MemoryChurnBench.getFromRepoStringConcat 1000 thrpt 5 35.975 ± 0.039 ops/s 12MemoryChurnBench.getFromRepoStringConcat 10000 thrpt 5 3.762 ± 0.001 ops/s 13 14Java 17 15Benchmark (nameLength) Mode Cnt Score Error Units 16MemoryChurnBench.getFromRepoNewObject 5 thrpt 5 10286.933 ± 709.515 ops/s 17MemoryChurnBench.getFromRepoNewObject 10 thrpt 5 10687.213 ± 167.468 ops/s 18MemoryChurnBench.getFromRepoNewObject 100 thrpt 5 10675.250 ± 1194.855 ops/s 19MemoryChurnBench.getFromRepoNewObject 1000 thrpt 5 11001.058 ± 1460.443 ops/s 20MemoryChurnBench.getFromRepoNewObject 10000 thrpt 5 10464.001 ± 80.074 ops/s 21MemoryChurnBench.getFromRepoStringConcat 5 thrpt 5 3036.421 ± 524.678 ops/s 22MemoryChurnBench.getFromRepoStringConcat 10 thrpt 5 3133.770 ± 401.871 ops/s 23MemoryChurnBench.getFromRepoStringConcat 100 thrpt 5 2291.642 ± 122.767 ops/s 24MemoryChurnBench.getFromRepoStringConcat 1000 thrpt 5 238.308 ± 1.260 ops/s 25MemoryChurnBench.getFromRepoStringConcat 10000 thrpt 5 28.638 ± 0.278 ops/s Conclusion From the results above, we can draw the following conclusions:\nThe bigger the strings that are used for concatenation, the bigger the memory churn is. Java 17 has an order of magnitude better performance for string concatenation compared to java 8. Optimized version of code, without string concatenation, performs much better than string concatenation. This statement is true for all versions of Java. Java 17 has much performs better when it comes to garbage collection. For more stuff like this, you can follow me on Twitter, LinkedIn, or visit my website.\nAppendix: Java Code Used for Benchmarking At the end, here is the code that I used to perform benchmarks:\n1@Fork(warmups = 0, value = 1) 2@BenchmarkMode(Mode.Throughput) 3@OutputTimeUnit(TimeUnit.SECONDS) 4@Measurement(time = 10, iterations = 5) 5@Warmup(iterations = 5, time = 1) 6public class MemoryChurnBench { 7 private static final int TOTAL_NO_ITEMS = 10_000; 8 9 @State(Scope.Benchmark) 10 public static class InputParams { 11 PersonRepository personRepository = new PersonRepository(); 12 @Param({\u0026#34;5\u0026#34;, \u0026#34;10\u0026#34;, \u0026#34;100\u0026#34;, \u0026#34;1000\u0026#34;, \u0026#34;10000\u0026#34;}) 13 private int nameLength; 14 15 List\u0026lt;Pair\u0026lt;String, String\u0026gt;\u0026gt; searchPersonNamesList; 16 17 public InputParams() { 18 } 19 20 @Setup(Level.Trial) 21 public void createRandomList() { 22 searchPersonNamesList = new ArrayList\u0026lt;\u0026gt;(TOTAL_NO_ITEMS); 23 Set\u0026lt;String\u0026gt; generated = new HashSet\u0026lt;\u0026gt;(); 24 for (int i = 0; i \u0026lt; TOTAL_NO_ITEMS; i++) { 25 String firstName = getUniqueFirstName(generated); 26 String lastName = getUniqueLastName(generated); 27 searchPersonNamesList.add(Pair.of(firstName, lastName)); 28 } 29 } 30 31 private String getUniqueLastName(Set\u0026lt;String\u0026gt; generated) { 32 String firstName; 33 do { 34 firstName = getLastName(nameLength); 35 } 36 while (generated.contains(firstName)); 37 return firstName; 38 } 39 40 private String getUniqueFirstName(Set\u0026lt;String\u0026gt; generated) { 41 String firstName; 42 do { 43 firstName = getFirstName(nameLength); 44 } 45 while (generated.contains(firstName)); 46 return firstName; 47 } 48 49 public String getFirstName(int i) { 50 return getString(\u0026#34;F:\u0026#34;, i); 51 } 52 53 private static String getString(String prefix, int length) { 54 return prefix + RandomStringUtils.random(length, true, true); 55 } 56 57 public String getLastName(int i) { 58 return getString(\u0026#34;L:\u0026#34;, i); 59 } 60 61 public void setNameLength(int nameLength) { 62 this.nameLength = nameLength; 63 } 64 } 65 66 @Benchmark 67 public void getFromRepoStringConcat(InputParams params, Blackhole b) { 68 for (int i = 0; i \u0026lt; TOTAL_NO_ITEMS; i++) { 69 Pair\u0026lt;String, String\u0026gt; personFullName = params 70 .searchPersonNamesList 71 .get(i); 72 Person repositoryPerson = params 73 .personRepository 74 .findPerson1(personFullName.left(), personFullName.right()); 75 b.consume(repositoryPerson); 76 } 77 } 78 79 @Benchmark 80 public void getFromRepoNewObject(InputParams params, Blackhole b) { 81 for (int i = 0; i \u0026lt; TOTAL_NO_ITEMS; i++) { 82 Pair\u0026lt;String, String\u0026gt; personFullName = params 83 .searchPersonNamesList 84 .get(i); 85 Person repositoryPerson = params 86 .personRepository 87 .findPerson2(personFullName.left(), personFullName.right()); 88 b.consume(repositoryPerson); 89 } 90 } 91} ","link":"https://www.professionaldev.pro/post/java/memory_churn/","section":"post","tags":["java","peformance"],"title":"What Is Memory Churn and How to Avoid It in Java"},{"body":"Extracting archive files without controlling resource consumption is security-sensitive and can lead to denial of service.\nOur code executes on servers, but you should know that servers have limits. Based on this, check how many hardware resources your code can consume. Resources are CPU, RAM, disk, network... Understand those limits and, based on those limits, put thresholds in your code.\nHere are some important thresholds you should put in a code when extracting a zip archive.\n1. Limit the size of an extracted archive This is the most basic and critical threshold you should introduce when extracting a zip archive.\nHow to limit extracted size? You can check the size after you extract the archive, but this can be too late. If the extracted content is too big, it can drain your server resources.\nYou should include a size threshold check in the extraction process. Each time you extract and get some bytes from an archive, compare the total extracted size with the threshold.\nZip archives also have metadata. In metadata, you can find each entry's file size. But, as another application wrote metadata, this information doesn't need to be correct.\n2. Limit the number of entries in an archive Each file system has some limitations in terms of the number of files and directories that can handle in a directory/partition/hard drive... Those limits are usually huge numbers. Even so, this is a limited resource. In the Sonar article there is the following statement:\nToo many entries in an archive, can lead to inodes exhaustion of the system.\nInodes are indexes that point to the actual location of a file on a disk. If you have too many small files on the file system, it can drain out available inodes. Consequently, your system cannot store new files.\nInodes are used on the linux/unix file systems, but a similar limit also exists for windows. Source: stackoverflow.\n3. Protect yourself against zip bomb Successful Zip Bomb attacks occur when an application expands untrusted archive files without controlling the size of the expanded data, which can lead to denial of service. Source: Sonar article\nA Zip bomb is usually a malicious archive file of a few kilobytes with extracted content measured in gigabytes. To achieve this extreme compression ratio, attackers will compress irrelevant data (e.g., a long string of repeated bytes).\nYou can have a total archive file size threshold, but you should always strive to fail fast. The compression ratio can give you an idea of whether the data in the archive is relevant. The data compression ratio for most legit archives is 1 to 3. And in the example from the sonar article threshold is 10, which should be a good value.\nHere you can find example zip bomb files.\n4. Sanitize zip files Sure, here is the revised version in Markdown format with grammatical corrections:\nRun a virus scan on a zip file before performing any other action.\nThis can help prevent malicious files from being uploaded and potentially causing harm to the server or other users.\nCheck the content of the zip archive.\nThe content of a zip archive is stored sequentially as a sequence of bytes, followed by a central directory at the end of the file. The central directory contains metadata about all zip content. For each file, it contains a relative path inside the zip archive. This can create a problem as it can allow a ZipSlip attack because a user can enter the file name in the form ..\\..\\file, leading to the extraction of a zip file in an unwanted location.\nTo fix this, add the following check: the canonical path of the file does not start with the path of the target directory.\n1File file = new File(extractDirPath, zipEntry.getName()); 2if( !file.getCanonicalPath().startsWith(extractDirPath) ) { 3 throw new SecurityException(\u0026#34;ZipEntry not within target directory!\u0026#34;); 4} Check file extensions inside the zip file.\nThey should match the file extensions that are allowed by your application.\nChange the filename to something generated by the application.\nYou can use UUID to generate the file name. This can help prevent conflicts between files and make it harder for an attacker to predict filenames, which can be useful in certain types of attacks.\n5. Don't extract archives inside an archive This will also create the same problem as with extracting folder content. It is better to go with one of the following options:\nTreat archive files in the archive as all other files - don't extract them. Check with the business if you can forbid the archive files inside an archive. You can ask this question because compressing an already compressed archive will not lead to a smaller file. You can forbid archive entries by checking entry extension and mime type during the extraction process. 6. Don't rely only on archive entry metadata Archives contain metadata that you can read during the extraction process. Compressing application wrote this data in an archive during the compression process.\nThe problem with this is that if you rely only on those properties, you need to trust a compressing application that is not in your control. Hackers can create fake metadata and crush your application.\nBe careful when using zip archive metadata. Always ask yourself how attackers can abuse it to hack your extraction algorithm.\nSources: Wikipedia The Risk of Archive Extraction OWASP - File Upload Cheat Sheet ","link":"https://www.professionaldev.pro/post/java/extract_zip_stream_best_practices/","section":"post","tags":["zip","java"],"title":"Best Practices That You Should Follow When Extracting Zip Archive in Java"},{"body":"","link":"https://www.professionaldev.pro/tags/zip/","section":"tags","tags":null,"title":"zip"},{"body":"","link":"https://www.professionaldev.pro/tags/carrier/","section":"tags","tags":null,"title":"carrier"},{"body":"","link":"https://www.professionaldev.pro/categories/carrier/","section":"categories","tags":null,"title":"carrier"},{"body":"Here are several reasons: We work in teams.\nThe better we know how to communicate with people in a team, the better we will be. Remember, your whole company is one big team. We use programming languages to communicate between each other.\nHardware doesn't need a programming language to process our intentions. All languages compile at the end in machine code. Programming languages are here to help us communicate ideas. The cleaner(more readable) code you write, the more understandable it will be to other developers. This leads to more maintainable software. Soft skills usually apply to your private life, making you a better person. The root of the word Software is Soft :o\nThese days, teams create great and successful products, not one person. Learn how to be a team player. Where can you start? Here are several subjects that are very important: Learn how to listen. Learn active listening. This is one of the most essential skills to learn. And it applies to more than work. You can (and should) use this skill in your private life. Learn about leadership. Everyone is a leader in some sense. Google \u0026quot;Simon Sinek\u0026quot; - he is an excellent resource for this subject. Learn about ownership. The best workers are those who own the work assigned to their team. Learn about empathy. To understand how people around you feel like. Learn about assertive communication! You probably know about this. If not, ask your HR! They just love this subject. ;) ","link":"https://www.professionaldev.pro/post/soft_skills/","section":"post","tags":["carrier"],"title":"Why Should You, as a Software Developer, Learn Soft Skills?"},{"body":"","link":"https://www.professionaldev.pro/tags/concurrent-tests/","section":"tags","tags":null,"title":"concurrent tests"},{"body":"","link":"https://www.professionaldev.pro/tags/java-testing/","section":"tags","tags":null,"title":"java testing"},{"body":"","link":"https://www.professionaldev.pro/tags/junit-5/","section":"tags","tags":null,"title":"junit 5"},{"body":"","link":"https://www.professionaldev.pro/tags/junit-5-testing/","section":"tags","tags":null,"title":"junit 5 testing"},{"body":"","link":"https://www.professionaldev.pro/tags/junit-testing/","section":"tags","tags":null,"title":"junit testing"},{"body":"","link":"https://www.professionaldev.pro/tags/maven/","section":"tags","tags":null,"title":"maven"},{"body":"","link":"https://www.professionaldev.pro/tags/multithreaded-tests-in-java/","section":"tags","tags":null,"title":"multithreaded tests in java"},{"body":"","link":"https://www.professionaldev.pro/tags/parallel-tests/","section":"tags","tags":null,"title":"parallel tests"},{"body":"Introduction We need fast-build pipelines. This is because we need fast feedback from our pipeline in order to be more productive. If something is wrong with our code, then we want our pipeline to fail fast. To accomplish that, you may decide to allow parallel test execution for tests in your project. This post describes how to do that with Maven and Junit 5.\nGetting Started With Junit 5 and Maven To get started with Junit 5, you need to import the following dependency into the project:\n1 2\u0026lt;dependency\u0026gt; 3 \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; 4 \u0026lt;artifactId\u0026gt;junit-jupiter-engine\u0026lt;/artifactId\u0026gt; 5 \u0026lt;version\u0026gt;5.9.0\u0026lt;/version\u0026gt; 6 \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; 7\u0026lt;/dependency\u0026gt; At the time of writing this article, version 5.9.0 is the newest. You should check maven repository for a new version. Notice, that we imported the library in test scope as there is no need for this library to be in production code because we only use it for running tests.\nRunning Tests in Parallel Maven offers you two ways for running tests in parallel:\nBy starting multiple JVM instances By using multiple threads in the same JVM instance 1. By Using the Parallel Parameter The setup for parallel test execution is straightforward. Include the following parameter:\n1 2\u0026lt;forkCount\u0026gt;2\u0026lt;/forkCount\u0026gt; It must have a value greater than 1 to enable parallel execution. It is important to know that parallel execution is achieved by starting multiple JVM child processes. This has multiple consequences, and it can affect your decision to use this method. Maven starts multiple JVM instances, and this means that it consumes more memory. Each thread has its own memory space. But it accomplishes a greater level of test independence, as processes cannot share data. You can use this method when you need to use an in-memory database for your unit tests.\nThe parameter forkCount has a fixed positive integer value which represents the number of forks. Of course, it is usual that multiple developers work on the same project, and we don't know how many cores other developers will have on their machines. To better use hardware, it would be great if we can set this parameter to create a number of forks that depends on the number of cores on CPU. Maven supports this, and you can configure it in the following way:\n1 2\u0026lt;forkCount\u0026gt;1C\u0026lt;/forkCount\u0026gt; This configuration instructs Maven to create one fork for each core. If your CPU has two virtual threads per core, you can also enter 2Cto use most CPU resources. Or if you don't want to use 100% of your CPU, you can enter 0.5C which will instruct Maven to create forks for the half of CPU cores.\n2. By Setting up Junit Multi-Thread Execution Maven uses plugins for everything. This is also the case for running project tests. To run project tests, Maven uses the Surefire plugin. Unfortunately, it uses the old version of the plugin by default, which doesn't support Junit5 test execution. So, we need to import a newer version of this plugin to Maven. To include a newer version of this plugin, we need to configure Maven. We can do this by adding the following XML to the pom.xml:\n1 2\u0026lt;build\u0026gt; 3 \u0026lt;pluginManagement\u0026gt; 4 \u0026lt;plugins\u0026gt; 5 \u0026lt;plugin\u0026gt; 6 \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; 7 \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; 8 \u0026lt;version\u0026gt;3.0.0-M7\u0026lt;/version\u0026gt; 9 \u0026lt;configuration\u0026gt; 10 We will add configuration here 11 \u0026lt;/configuration\u0026gt; 12 \u0026lt;/plugin\u0026gt; 13 \u0026lt;/plugins\u0026gt; 14 \u0026lt;/pluginManagement\u0026gt; 15\u0026lt;/build\u0026gt; The XML is quite self-descriptive. We add new plugin that is in build-\u0026gt;pluginManagement-\u0026gt;plugins section of Maven pom.xml. We import the newest version of the surefire plugin at the time of writing this article. You can check this url for a newer version.\nNow, when we have a newer version of the surefire plugin loaded, we can continue to configure Maven for multithreaded test execution.\nIn the official Maven documentation, to achieve this, you need to use parallel, threadCount and useUnlimitedThreads parameters. I tried all these parameters, but nothing worked with Junit 5. By checking official documentation for Maven Surefire and Junit 5 we can conclude that Junit 5 supports a new multithreaded execution model (which is still in the preview phase). So, to configure multithreaded execution, we need to configure Junit5 as well. We can do this by setting up properties in the Maven configuration.\n1 2\u0026lt;configuration\u0026gt; 3 \u0026lt;properties\u0026gt; 4 \u0026lt;configurationParameters\u0026gt; 5 junit.jupiter.execution.parallel.enabled=true 6 ... 7 \u0026lt;/configurationParameters\u0026gt; 8 \u0026lt;/properties\u0026gt; 9\u0026lt;/configuration\u0026gt; Another option is to create junit-platform.properties file. Put it into test/resources folder, and put all the properties in that file instead of pom.xml.\nIt is enough to set ``junit.jupiter.execution.parallel.enabled=trueto allow parallel test execution. But, if you try running the test after setting up this parameter value totrue``` it will surprise you to see that tests are still running sequentially. The reason for this is that there are 2 strategies in which you can allow parallel execution in tests.\nJunit can assume that all tests are sequential, and only ones that are annotated with @Execution(CONCURRENT) will be executed in parallel. You can use this annotation on a class or test method to enable parallel execution. You can set parameter junit.jupiter.execution.parallel.mode.default = concurrent. Here, all tests will run in parallel by default except ones annotated with ExecutionMode.SAME_THREAD. How Many Threads Junit Will Use for Parallel Execution? This is also configurable property junit.jupiter.execution.parallel.config.strategy=dynamic and it also has default value dynamic. Strategy can be also fixed or custom. If we set up fixed value for this property, then we also need to set up the value for the junit.jupiter.execution.parallel.config.fixed.parallelism=\u0026lt;positive integer\u0026gt;. This is not a scalable solution, and we should use dynamic value to better utilize the hardware.\ndynamic property also has a connected property that influences how many threads JUnit will create. junit.jupiter.execution.parallel.config.dynamic.factor=\u0026lt;positive decimal number\u0026gt;. Factor to multiply by the number of processors/cores to determine the desired parallelism for the dynamic configuration strategy.\nConclusion Parallel test execution can speed up your build, but you must be careful when enabling it as it will not always provide you with better performance. This is because the tests may use the same resource, and they can wait for each other to access it. Because of that, it is always a good idea to test and check everything.\nAnother more significant problem with parallel tests is if the tests are not independent, there can be flaky tests. They are very hard to debug and identify.\nIf you liked this post, you can follow me on Twitter or LinkedIn for more content.\nResources Maven documentation\nJunit guide\nJUnit5 Parallel Execution of tests\n","link":"https://www.professionaldev.pro/post/java/java_multithread_junit5/","section":"post","tags":["java","maven","junit 5","testing","parallel tests","concurrent tests","junit testing","java testing","junit 5 testing","multithreaded tests in java"],"title":"Setting up Junit 5 Parallel Test Execution With Maven"},{"body":"","link":"https://www.professionaldev.pro/tags/testing/","section":"tags","tags":null,"title":"testing"},{"body":"","link":"https://www.professionaldev.pro/categories/testing/","section":"categories","tags":null,"title":"testing"},{"body":" Currently, I work as Senior Java Developer \u0026amp; Tech Lead at Badin Soft. With more than 8 years of professional experience as a Backend Developer and experience with software development since 2007. Worked with Java Web Services and relational databases on different fintech products. My focus is on helping professional Java developers to optimize their work and solve problems using best practices, new technologies and methodologies by writing clean, secure and performant code in agile environment, so they can build great products, solve complex problems, and become efficient and successful developers.\n","link":"https://www.professionaldev.pro/about/","section":"","tags":null,"title":"About Me"},{"body":"","link":"https://www.professionaldev.pro/tags/developers/","section":"tags","tags":null,"title":"developers"},{"body":"","link":"https://www.professionaldev.pro/tags/developers-biggest-problem/","section":"tags","tags":null,"title":"developers biggest problem"},{"body":"I have attended the workshop that is organized by the ITKonekt. The workshop was about efficient coding practices for best performance in Java. Java Champion Victor Rentea did a great job explaining all java internals, tools, hibernate, threads, collections, garbage collections, and much more. It was very enjoyable to learn from him, and I decided to ask him the question:\nWhat is the biggest problem that professional Java developers are facing today?\nHis initial answer was: \u0026quot;Not writing enough Java. They write Kubernetes scripts, dev-ops scripts, python scripts…\u0026quot;\nMy understanding of this is that those people are very experienced developers. He also mentioned that this is what his colleagues are working on currently. But if you are just starting your career, he has advice for you:\nIf you are starting a career, then you should learn the language and frameworks that you are using. You should master the language and frameworks. This is the first challenge. Don't be disgusted by the front-end technologies, try to be full stack after you master Java. This is after you are comfortable with Java. Go a bit full stack, not \u0026quot;full full\u0026quot; stack, but \u0026quot;just a bit\u0026quot; full stack. Try to learn more about what surrounds you. Learn some other language. Kotlin is a good one. Learn dev ops. Try to learn a bit of everything. He also gave me this advice:\nAs long as you are surrounded by people smarter than you, you are good. And as long as you love to work with your team, you are good. Doesn't matter project or technology, it can be a terrible legacy system. If you have super cool surroundings, and you keep learning, and you keep having fun, it's perfect.\nIf you liked this post, you can follow me on Twitter, or LinkedIn.\nVictor is a great trainer and one of the best presenters. You can find him at the following links:\nWebsite victorrentea.ro you can find a blog, recorded talks, plus other goodies. Bucharest-software-craftsmanship-community (3000+ developers). Twitter http://www.linkedin.com/in/victor-rentea-trainer[LinkedIn], https://fb.me/VictorRentea.ro[Facebook]. Victor's YouTube Channel - here you can find more recorded and live content, including coding katas, recordings of past meetups, and more. ","link":"https://www.professionaldev.pro/post/java/victor_biggest_problem_for_java_devs/","section":"post","tags":["java","developers","developers biggest problem"],"title":"I Asked a Java Champion: What Is the Biggest Problem That Professional Java Developers Are Facing Today?"},{"body":"Based on my previous experience, I created a list of 10 mistakes that developers made, preventing them from being a great developer. Here is the list:\n1. Not writing unit tests Developers that don’t write unit tests produce more bugs from the code they write and maintain. That leads to unstable products and client dissatisfaction.\nIf you are not familiar with writing unit tests, there are some resources to get started with:\nhttps://www.vogella.com/tutorials/Mockito/article.html https://www.baeldung.com/mockito-series https://www.softwaretestinghelp.com/mockito-tutorial/ 2. Not manually testing code Even if you completely cover your code with unit tests, there is still a chance that you missed something out. It happens in practice that some error pushes through.\nIt is always good practice to manually test code before pushing it for code review. By doing this, you will look at your solution from the client’s perspective. And not only that you can detect bugs, but you can also identify design problems in the development stage.\n3. Having the mindset “This will never happen” Developers often make mistakes when they write new code by thinking that certain scenarios in code will never happen. Eventually, it turns out that they are wrong. In those situations, applications can behave unpredictably and it can lead to bugs. Handle every scenario that code can go into.\nDefensive programming techniques will help you in that. If you are not familiar with defensive programming, you can check the following Pluralsight course: https://www.pluralsight.com/courses/defensive-programming-java\n4. Not asking for feedback and not giving feedback To improve yourself, regularly ask for feedback. You can as for feedback when you finish a ticket, or after finishing a project, or when you do a presentation… There is no bad time to ask for feedback.\nGive feedback to your colleagues. And not by telling them they are great even if you think they are not so good. Tell them areas where they can improve themselves. If the feedback is honest, they will appreciate you more.\n5. Not checking the performance of code Often, developers write their code, but they don’t check for performance. When code goes to production, it creates various problems. Poor performance can even crush the server.\n6. Writing long procedural code It is very easy to write long methods with a bunch of logic. By doing this, programmers put the same logic in many places. Projects with a lot of small methods have much greater code reusability and are much easier to maintain.\n7. Not being familiar with the tools Tools are extensions of your hands. The better you know them, the more productive you will be. You should be very familiar with the IDE you use.\nLearn shortcuts, they will make you much more productive. Try learning one shortcut to a day and create your personal cheat sheet.\nResearch plugins, usually you will find a plugin that will help you be even more productive. Plugins that will help you write better code in Intellij Idea are Sonar Lint, Spot bugs, and Code Metrics.\n8. Ignoring problems in code Developers that are working on the most successful products are changing the code all the time. Don’t be afraid to refactor code. If your code is unit tested, then there is a low probability of introducing a regression.\nBut, don’t stop there. Developers often ignore problematic code that is not part of their ticket. As a developer, you are responsible to maintain an application and keep it in good shape. Because of that, fix all problems that you find.\nThe best way to proceed with fixing the problem is to create a ticket and work on it with your team. The following story emphasizes why it is important not to ignore problems in code: https://blog.codinghorror.com/the-broken-window-theory/.\n9. Coding by accident Developers should NEVER do a code modification and push that code in production without understanding the consequences of it. Code can produce correct results for given test values. However, there can be scenarios where it can produce unpredicted results and create serious problems.\nCoding by accident often happens when developers use features from libraries that don’t completely understand. It can also happen when the developer solves the problem without understanding the solution.\n10. Not asking for help Developers are not very communicative people. They like to solve problems by themselves. The era where one developer creates a complete solution from start to end is over.\nDeveloping software is a team activity. When you encounter a problem during programming, try to solve it by yourself. But don’t waste too much time if you can’t figure out the solution. There is a high probability that some of your colleagues already encounter the same problem and know a solution.\nIf it is not the case, then you will get help and the team will understand that the problem is complex and that you need time to solve it. Involving more people will help you resolve complex problems faster. Developers that don’t ask for help, usually spend too much time on a ticket.\nHelp others when you see they have problems with their ticket. As a result, the team will be more productive and people will like you more.\nIf you like this content, you can follow me on Twitter or LinkedIn.\n","link":"https://www.professionaldev.pro/post/java/teen_mistakes/","section":"post","tags":["java","java developers","mistakes","java mistakes","developers mistakes","successful developers"],"title":"10 Mistakes That Java Developers Make That Prevent Them From Being Successful Developers"},{"body":"","link":"https://www.professionaldev.pro/tags/developers-mistakes/","section":"tags","tags":null,"title":"developers mistakes"},{"body":"","link":"https://www.professionaldev.pro/tags/java-developers/","section":"tags","tags":null,"title":"java developers"},{"body":"","link":"https://www.professionaldev.pro/tags/java-mistakes/","section":"tags","tags":null,"title":"java mistakes"},{"body":"","link":"https://www.professionaldev.pro/tags/mistakes/","section":"tags","tags":null,"title":"mistakes"},{"body":"","link":"https://www.professionaldev.pro/tags/successful-developers/","section":"tags","tags":null,"title":"successful developers"},{"body":"","link":"https://www.professionaldev.pro/tags/functional-programming/","section":"tags","tags":null,"title":"functional programming"},{"body":"","link":"https://www.professionaldev.pro/categories/functional-programming/","section":"categories","tags":null,"title":"functional programming"},{"body":"","link":"https://www.professionaldev.pro/tags/getting-started-with-functional-programming-in-java/","section":"tags","tags":null,"title":"getting started with functional programming in java"},{"body":"As a Java developer, I always look for ways to improve my coding skills. I heard about functional programming (FP) back when I was a student. Then, FP was not very popular and most developers considered code written using FP to be slow.\nTime has changed and today FP is very popular. Some developers are considering it to be the future of how developers write code.\nFP is the most useful concept that I learned in the last several years. It helped me to become a better developer. I started writing cleaner code with fewer bugs.\nHere are the immediate benefits I got from FP: I sharpen my skills related to using Java Streams. It is much easier for me to work with Streams when I understand FP concepts. My functions that follow FP concepts are easy to understand and maintain. There is no risk to using them in a concurrent environment. The reusability of those functions is much greater. If you are not a fan of “if” and “for” statements, then you will like FP. It can help you write more understandable code that doesn’t include those statements. I can do more with fewer lines of code. The FP is a declarative paradigm. With FP you describe what you want, rather than how to get it. This means that the code is more readable, reusable, and it is easier to maintain. So, how I started with functional programming? We can find a ton of material on the internet related to FP. I usually like to watch video material when I want to learn about a new subject.\nI attended an online presentation hold by Venkat Subramaniam. It was a great and inspiring presentation. The good news is that you can watch it on the following link: Functional Programming with Java 8.\nAfter that presentation, I gained an interest in FP, so I decided to study more about FP. I watched the Pluralsight course Functional Programming: The Big Picture. This course helped me understand the big picture of FP and why it matters.\nThe next natural step was to check if there is a course that's subject is related to implementing FP concepts in Java. I found the course Applying Functional Programming Techniques in Java. It is a great course. It helped me learn, understand, and apply new FP concepts in Java.\nAfter watching those courses, I wanted to know more about Monads. They are very important in FP. Following two videos helped me understand Monads: Brian Beckman: Don't fear the Monad and What the ƒ is a Monad?.\nLearn from Haskell I started learning Haskell. At the end of Venkat Subramaniam's presentation, I asked him if learning Haskell will help me better understand FP concepts. The answer was something like: “Not only Haskell helped me to understand FP, Haskell LEARNED ME how to write good code”. Some developers say that learning Haskell is like learning programming again from scratch. I learned great stuff from Haskell, and it was easy and fun to get started with it. Learn You a Haskell for Great Good! is a great tutorial to get started with Haskell.\nLearn some FP language If you don’t like Haskell, don’t worry, you can learn another FP language. You can check Kotlin, Scala, F#, Clojure, Elixir, Erlang...\nIf you want to stick with JVM, then you can learn Kotlin, Scala, or Clojure. Kotlin and Scala support functional and OOP paradigms and Java developers can easily get started with them. Kotlin is a new language, Spring framework supports it, and some Java developers started switching to it. Because of those reasons, I suggest trying Kotlin.\nClojure is a modern Lisp variant that runs on JVM. It is useful to know that Lisp is the oldest FP language. Also, Uncle Bob is using Clojure.\nMy experience with Functional Programming To get familiar with FP, I studied and applied in practice FP concepts. The most important concepts that I learned are:\nImmutability - I have as many as possible immutable objects in my codebase. This leads to fewer places where I can change the state of the program. And that leads to fewer bugs. Referential transparency - I write as much as possible pure functions. Those functions are like mathematical functions. For the same input, they always have the same output. Pure and unpure functions - before FP, I was not aware of this concept. Now, I separate pure functions from unpure. It allowed me to easier test the code and improved my code reusability. Function Composition - promotes better code readability and it is easier to write code by composing functions. Curried Functions - brilliant concept, but it is not natural to use it in Java like in other FP languages. I don’t use them for now. Lazy evaluation - evaluate values when they are needed. Lambdas are the way to do a lazy evaluation in Java. Higher-Order functions - receive other functions as parameters in existing functions. Those functions are usually utility functions. Map, filter, reduce pattern - Java Stream API implements this pattern. Monads - helped me understand how to handle unpure functions safely. Optional class - FP provides an efficient solution on how to work with nullable objects. I always return the Optional object instead of null in a method that can return null. Railway programming - helped me understand how Stream API and Optional class works. Using FP in code doesn't prevent us, developers, from writing bad code. We still need to write unit tests, have a good understanding of our task and our codebase. We still need to apply all the best practices that we learned in the past. FP is promoting good practices and makes it easier for us developers to write good and maintainable code.\nI plan to further learn about FP and Java. I will continue to write posts on this subject. For updates, you can follow me on Twitter or LinkedIn.\n","link":"https://www.professionaldev.pro/post/java/start_fp_in_java/","section":"post","tags":["functional programming","java","learn functional programming","getting started with functional programming in java"],"title":"How do I become proficient with functional programming in Java"},{"body":"","link":"https://www.professionaldev.pro/tags/learn-functional-programming/","section":"tags","tags":null,"title":"learn functional programming"},{"body":"","link":"https://www.professionaldev.pro/post/java/mssqlserver/","section":"post","tags":null,"title":""}]